{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc08e386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import plotnine as p9\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBTEXT_DIR = '.'\n",
    "MAUVE_SCORES_DIR = 'data/mauve_scores_full'\n",
    "TENSOR_STORAGE_DIR = '/local/home/meistecl/tensors'\n",
    "NUM_HYPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261193a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_factors = ['adapter', 'param', 'model', 'dataset']\n",
    "variation_factors = base_factors + ['comparison']\n",
    "corr_factors = ['comparison', 'model']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed382cc",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559402f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "webtext_dir_name = os.path.join(WEBTEXT_DIR, 'data')\n",
    "webtext_human = utils.clean_text(utils.load_gpt2_dataset(os.path.join(webtext_dir_name,'webtext.test.jsonl'), num_examples=NUM_HYPS)) # human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9814f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitext_test = utils.clean_text(load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettify_adapter_names(x):\n",
    "    x = x.title()\n",
    "    if 'Top' in x:\n",
    "        x = x[:-1] + x[-1].upper()\n",
    "    return x\n",
    "\n",
    "def prettify_model_names(x):\n",
    "    mapping = {'gpt-neo': 'GPT-Neo', 'gpt2':'GPT-2', 'gpt2-large':'GPT-2 Large'}\n",
    "    return mapping.get(x)\n",
    "\n",
    "\n",
    "mauve_scores = pd.DataFrame()\n",
    "for d in os.listdir(MAUVE_SCORES_DIR):\n",
    "    if not os.path.isdir(os.path.join(MAUVE_SCORES_DIR, d)):\n",
    "        continue\n",
    "    params = d.split('_')\n",
    "    ms = utils.get_mauve_scores_from_files(os.path.join(MAUVE_SCORES_DIR, d))\n",
    "    ms['laplace'] = int(d[-1])\n",
    "    ms['mauve_eval'] = params[-2]\n",
    "    ms['model'] = params[-4]\n",
    "    ms['dataset'] = params[-3]\n",
    "    ms.loc[ms['adapter'] == 'ancestral', 'param'] = np.nan\n",
    "    columns = ['mauve_scores', 'js_scores'] \n",
    "    if 'laplace1' in d:\n",
    "        columns +=  ['forward_scores', 'backward_scores']\n",
    "    mauve_scores = pd.concat([mauve_scores, ms.explode(columns)])\n",
    "\n",
    "\n",
    "mauve_scores = mauve_scores.apply(pd.to_numeric, errors='ignore').reset_index(drop=True)\n",
    "mauve_scores['adapter'] = mauve_scores['adapter'].apply(prettify_adapter_names)\n",
    "mauve_scores['model'] = mauve_scores['model'].apply(prettify_model_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275e5fcd",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cfddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, AutoTokenizer, AutoModelForCausalLM\n",
    "device = \"cuda\"\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id, pad_token_id=tokenizer.eos_token_id).to(device)\n",
    "\n",
    "device2 = \"cuda:1\"\n",
    "model2 = GPT2LMHeadModel.from_pretrained(\"gpt2-large\").to(device2)\n",
    "tokenizer2 = GPT2TokenizerFast.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "device3 = \"cpu\"\n",
    "model3 = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\").to(device3)\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794bae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:2\"\n",
    "# model_id = \"EleutherAI/gpt-neo-125M\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, pad_token_id=tokenizer.eos_token_id).to(device)\n",
    "\n",
    "# device2 = \"cuda:3\"\n",
    "# model2_id = \"EleutherAI/gpt-neo-1.3B\"\n",
    "# model2 = AutoModelForCausalLM.from_pretrained(model2_id).to(device2)\n",
    "# tokenizer2 = AutoTokenizer.from_pretrained(model2_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa0e32a",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e5ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling adapter methods\n",
    "from transformers import TypicalLogitsWarper, TopKLogitsWarper, TopPLogitsWarper, TemperatureLogitsWarper, LogitsProcessorList\n",
    "from utils import EtaWarper\n",
    "\n",
    "ADAPTERS = [(EtaWarper, [0.1, 0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00001]),\n",
    "          (TopKLogitsWarper, [1, 5, 10, 30, 50, 100, 500, 1000, 10000]),\n",
    "          (TopPLogitsWarper, [0.3, 0.5, 0.7, 0.8, 0.9, 0.95, 0.99, 0.999]),\n",
    "          (TypicalLogitsWarper, [0.3, 0.5, 0.7, 0.8, 0.9, 0.95, 0.99, 0.999 ]),\n",
    "           (TemperatureLogitsWarper, [0.3, 0.5, 0.7, 0.9, 0.95, 1.05, 1.2])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_wrapper(log_probs, reference_log_probs, reference=None):\n",
    "    js, (forward_js, backward_js) = metrics.jensen_shannon(reference_log_probs, log_probs)\n",
    "    m = metrics.Metrics(tvd=metrics.tvd(torch.exp(log_probs), torch.exp(reference_log_probs)).cpu().numpy(),\n",
    "                   forward_kl=metrics.forward_kl(reference_log_probs, log_probs).cpu().numpy(),\n",
    "                   backward_kl=metrics.backward_kl(reference_log_probs, log_probs).cpu().numpy(),\n",
    "                  js=js.cpu().numpy(), forward_js=forward_js.cpu().numpy(), backward_js=backward_js.cpu().numpy(),\n",
    "                  token_counts=len(log_probs),\n",
    "                  entropy=metrics.entropy(log_probs).cpu().numpy(),\n",
    "                  reference_entropy=metrics.entropy(reference_log_probs).cpu().numpy(),\n",
    "                  cross_entropy=metrics.cross_entropy(reference_log_probs, log_probs).cpu().numpy(),\n",
    "                  eps_forward_kl=metrics.eps_forward_kl(reference_log_probs, log_probs).cpu().numpy(),\n",
    "                  eps_backward_kl=metrics.eps_backward_kl(reference_log_probs, log_probs).cpu().numpy()\n",
    "                     )\n",
    "    if reference is not None:\n",
    "        m = m._replace(inf_counts=metrics.inf_counts(log_probs, reference).item())            \n",
    "        m = m._replace(perplexity=metrics.perplexity(log_probs, reference).item())\n",
    "        m = m._replace(eps_perplexity=metrics.epsilon_perplexity(log_probs, reference).item())\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d928f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 10\n",
    "cur_text = webtext_human\n",
    "\n",
    "all_results = {}\n",
    "all_results['reference'] = defaultdict(lambda: defaultdict(list))\n",
    "all_results['reference_model'] = defaultdict(lambda: defaultdict(list))\n",
    "all_results['reference_model2'] = defaultdict(lambda: defaultdict(list))\n",
    "for t in tqdm(cur_text[:num_examples]):\n",
    "    logits, labels = utils.get_model_outputs(t, model, tokenizer)\n",
    "    \n",
    "    logits_model2, _ = utils.get_model_outputs(t, model2, tokenizer2)\n",
    "    logits_model2 = logits_model2.to(logits.device)\n",
    "    \n",
    "    logits_model3, _ = utils.get_model_outputs(t, model3, tokenizer3)\n",
    "    logits_model3 = logits_model3[:, :logits.shape[-1]].to(logits.device)\n",
    "    \n",
    "    p = torch.nn.functional.one_hot(labels.view(-1), num_classes=logits.shape[1])\n",
    "    log_p = torch.log(p.float())\n",
    "    for warper, hyperparam in ADAPTERS:\n",
    "        all_results['reference'][warper.__name__]['-'].append(compute_metrics_wrapper(logits, log_p, labels))\n",
    "        all_results['reference_model'][warper.__name__]['-'].append(compute_metrics_wrapper(logits, logits_model2))\n",
    "        all_results['reference_model2'][warper.__name__]['-'].append(compute_metrics_wrapper(logits, logits_model3))\n",
    "        for h in hyperparam:\n",
    "            processor = LogitsProcessorList()\n",
    "            processor.append(warper(h))\n",
    "            # Labels are input to processor but arent actually used. Could even put arbitrary list of token ids...\n",
    "            shift_logits = processor(\n",
    "              labels.squeeze(0), logits.squeeze(0)).unsqueeze(0)\n",
    "            shift_logits = torch.nn.functional.log_softmax(shift_logits.view(-1, shift_logits.size(-1)), dim=1)\n",
    "            all_results['reference'][warper.__name__][h].append(compute_metrics_wrapper(shift_logits, log_p, labels))        \n",
    "            all_results['reference_model'][warper.__name__][h].append(compute_metrics_wrapper(shift_logits, logits_model2))        \n",
    "            all_results['reference_model2'][warper.__name__][h].append(compute_metrics_wrapper(shift_logits, logits_model3))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb059da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_df = utils.dict_to_df(all_results)\n",
    "\n",
    "all_results_df['sequence_coverage'] = (all_results_df['inf_counts'] == 0.).astype(int)\n",
    "all_results_df['token_coverage'] = (all_results_df['token_counts'] - all_results_df['inf_counts'])/all_results_df['token_counts']\n",
    "all_results_df['eps_backward_cross_ent'] = all_results_df['entropy'] + all_results_df['eps_backward_kl']\n",
    "all_results_df['backward_cross_ent'] = all_results_df['entropy'] + all_results_df['backward_kl']\n",
    "all_results_df['eps_forward_cross_ent'] = all_results_df['reference_entropy'] + all_results_df['eps_forward_kl']\n",
    "all_results_df['model'] = model_id\n",
    "all_results_df['dataset'] = \"webtext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "def nansem(x):\n",
    "    try: \n",
    "        return sem(x.explode())\n",
    "    except:\n",
    "        return np.nan\n",
    "grouped = all_results_df.groupby(variation_factors, dropna=False).aggregate(lambda x: np.mean(x.explode()))\n",
    "grouped_sem = all_results_df.groupby(variation_factors, dropna=False).aggregate(nansem)\n",
    "grouped = grouped.join(grouped_sem, rsuffix='_se')\n",
    "grouped = grouped.reset_index()\n",
    "grouped['adapter'] = grouped['adapter'].apply(lambda x: x.replace('Logits', '').replace('Warper', ''))\n",
    "grouped['param'] =  pd.to_numeric(grouped['param'], errors='coerce')\n",
    "grouped['model'] = grouped['model'].apply(prettify_model_names)\n",
    "grouped.loc[grouped['adapter'] == 'TopK','param'] = grouped.loc[grouped['adapter'] == 'TopK','param'].astype('Int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25b223",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf301b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "merged = grouped.merge(mauve_scores.loc[mauve_scores['laplace'] == 1], on=base_factors, suffixes=['', '_mauve'])\n",
    "corrs = merged.groupby(corr_factors).corr(method='spearman', numeric_only=True)[['mauve_scores', 'forward_scores', 'backward_scores']].reset_index(names=corr_factors+['att'])\n",
    "corrs_std = merged.groupby(corr_factors).corr(method=lambda x, y: spearmanr(x, y)[1], numeric_only=True)[['mauve_scores', 'forward_scores', 'backward_scores']].reset_index(names=corr_factors+['att'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e59cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "atts = ['backward_cross_ent','tvd', 'backward_kl', 'eps_forward_kl', 'eps_backward_kl','js', 'sequence_coverage', 'entropy']\n",
    "corrs_std.loc[corrs['att'].isin(atts) & corrs['comparison'].isin(['reference', 'gptj'])].sort_values(by=['mauve_scores'])#.to_csv('corr_pvals.csv')\n",
    "corrs.loc[corrs['att'].isin(atts) & corrs['comparison'].isin(['reference', 'gptj'])].sort_values(by=['mauve_scores'])#.to_csv('corrs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bce826",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9777f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'reference': 'Empirical',\n",
    "                 'gpt2': 'GPT2-XL',\n",
    "                 'gpt_neo': 'GPT-Neo (1.5B)',\n",
    "                 'gptj': 'GPT-J',\n",
    "                 }\n",
    "metric_mapping = {'tvd': 'TVD',\n",
    "                 'js': 'Jensen-Shannon',\n",
    "                  'entropy': 'Entropy',\n",
    "                  'backward_kl': \"Reverse KL\",\n",
    "                  'eps_perplexity': r'$\\epsilon$'+\"-Perplexity\",\n",
    "                  'eps_forward_kl': \"Forward KL\\n\",\n",
    "                  'backward_cross_ent': \"Reverse Cross-Entropy\",\n",
    "                  'eps_forward_cross_ent': \"Forward Cross-Entropy\",\n",
    "                  'eps_backward_kl': r'$\\epsilon$'+\"-Reverse KL\\n\",\n",
    "                  'token_coverage': \"Token Coverage (%)\"\n",
    "                 }\n",
    "def labeller(x):\n",
    "    return label_mapping.get(x, x)\n",
    "\n",
    "def equal_breaks(n, r=2, min_size=0.5):\n",
    "    def create_breaks(x):\n",
    "        min_ = min(x)\n",
    "        max_ = max(x)\n",
    "        cur_delt = max_ - min_\n",
    "        if cur_delt < min_size:\n",
    "            max_ += (min_size - cur_delt)/2\n",
    "            min_ -= (min_size - cur_delt)/2\n",
    "        delta = np.round((max_ - min_)/n, r)\n",
    "        d = list(np.arange(np.floor(min_ * 10**r)/10**r, max_, delta))\n",
    "        return d\n",
    "    return create_breaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2030f820",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('mode.use_inf_as_na', True):\n",
    "    grouped.loc[pd.isna(grouped['backward_cross_ent']),'backward_cross_ent'] = grouped.loc[pd.isna(grouped['backward_cross_ent']),'eps_backward_cross_ent'] \n",
    "    grouped.loc[pd.isna(grouped['backward_kl']),'backward_kl'] = grouped.loc[pd.isna(grouped['backward_kl']),'eps_backward_kl'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fefbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'eps_forward_cross_ent'\n",
    "dataset = 'webtext'\n",
    "filtered = grouped.loc[(grouped['dataset'] == dataset)]#.dropna(subset=[metric])\n",
    "no_adapter = filtered.loc[pd.isna(filtered['param'])]\n",
    "adapters = filtered.drop(no_adapter.index, errors='ignore', axis=0).apply(pd.to_numeric, errors='ignore')\n",
    "adapters = adapters.loc[~adapters['param'].isin([0.1, 1.])]\n",
    "fig = (p9.ggplot(data=adapters,#.loc[adapters[metric]<10],\n",
    "               mapping=p9.aes(x='factor(param)', y=metric, color='model'))\n",
    "         + p9.theme(text=p9.element_text(size=9, family='serif'), \n",
    "               axis_text_x=p9.element_text(size=8, angle=45),\n",
    "               axis_text_y=p9.element_text(size=8),\n",
    "               strip_text_y=p9.element_text(size=8),\n",
    "               aspect_ratio=0.8,\n",
    "               legend_position='top',\n",
    "                strip_background = p9.element_rect(fill=\"white\"))\n",
    "        + p9.geom_point(size=2)\n",
    "        + p9.geom_errorbar(mapping=p9.aes( ymin=metric + '-1.96*' + metric + '_se', ymax=metric + '+1.96*' + metric + '_se'), width=0.1)\n",
    "        + p9.facet_grid('comparison~adapter', scales='free', labeller=labeller)\n",
    "        + p9.geom_hline(mapping=p9.aes(yintercept=metric, color='model'), data=no_adapter, linetype='dashed')\n",
    "        + p9.labs(x=\"Adapter Parameter\", color=\"\", y=metric_mapping.get(metric, metric))\n",
    "        + p9.scale_x_discrete(labels=lambda x: [str(int(y)) if float(y) >= 5 else str(y) for y in x])\n",
    "\n",
    ").draw(show=False)\n",
    "#fig.savefig(metric+ '_' + dataset +'.png', dpi=300, bbox_inches = \"tight\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4adee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric='mauve_scores'\n",
    "laplace = 1\n",
    "mauve_scores_subet = mauve_scores.loc[mauve_scores['laplace']== laplace][base_factors + [metric, 'mauve']].drop_duplicates()\n",
    "gptj = grouped.loc[grouped['comparison'] == 'gptj']\n",
    "reference = grouped.loc[grouped['comparison'] == 'reference']\n",
    "\n",
    "df = gptj\n",
    "mauve_adapters = df.loc[~pd.isna(df['param'])].merge(mauve_scores_subet, on=base_factors, how='left')\n",
    "mauve_ancestral = df.loc[pd.isna(df['param'])].merge(mauve_scores_subet[mauve_scores_subet.columns.drop('adapter')], on=[n for n in base_factors if n != 'adapter'], how='left')\n",
    "mauve_df = pd.concat([mauve_ancestral, mauve_adapters])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4aa65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mauve Scores figure\n",
    "filtered = mauve_df.apply(pd.to_numeric, errors='ignore').dropna(subset=[metric]).reset_index(drop=True) #.drop_duplicates(subset=['model','adapter', 'param', 'dataset'])\n",
    "subset = filtered#.loc[filtered['dataset'] == 'webtext']\n",
    "no_adapter = subset.loc[pd.isna(subset['param'])]\n",
    "adapters = subset.drop(no_adapter.index, errors='ignore', axis=0)\n",
    "adapters = adapters.loc[((adapters['param'] != 1.) & (adapters['param'] != 0.1))]\n",
    "fig = (p9.ggplot(data=adapters.sort_values(by=['param']),\n",
    "               mapping=p9.aes(x='factor(param)', y=metric, color='model'))\n",
    "         + p9.theme(text=p9.element_text(size=9, family='serif'), \n",
    "               axis_text_x=p9.element_text(size=8, angle=45),\n",
    "               axis_text_y=p9.element_text(size=8),\n",
    "               strip_text_y=p9.element_text(size=8),\n",
    "               aspect_ratio=0.8,\n",
    "               legend_position='top')\n",
    "        + p9.stat_summary(fun_data = 'mean_cl_boot', size=0.5, fun_args={'confidence_interval':0.95})\n",
    "        + p9.facet_grid('dataset~adapter', scales='free', labeller=labeller)\n",
    "        + p9.geom_hline(mapping=p9.aes(yintercept='mauve', color='model'), data=no_adapter, linetype='dashed')\n",
    "        + p9.labs(x=\"Adapter Parameter\", color=\"\", y=metric_mapping.get(metric, metric.split('_')[0].title()))\n",
    "        + p9.scale_x_discrete(labels=lambda x: [str(int(y)) if float(y) >= 5 else str(y) for y in x])\n",
    "\n",
    ").draw(show=False)\n",
    "fig.savefig(metric+'_wide.png', dpi=300, bbox_inches = \"tight\")\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
